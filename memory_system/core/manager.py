import asyncio
import uuid
from typing import Dict, List, Optional, Any
from datetime import datetime
from .working_memory import WorkingMemory
from .episodic_memory import EpisodicMemory
from .semantic_memory import SemanticMemory
from .compressor import compressor
from .extractor import extractor
from .cluster import ConceptCluster
from config import settings
from utils.logger import logger
from utils.tokenizer import tokenizer


class MemoryManager:
    """
    è®°å¿†ç³»ç»Ÿæ ¸å¿ƒç®¡ç†å™¨
    è´Ÿè´£åè°ƒä¸‰å±‚è®°å¿†ç³»ç»Ÿï¼Œä¸ºæ¯ä¸ª Agent ç»´æŠ¤ç‹¬ç«‹çš„è®°å¿†å®ä¾‹
    """

    _instances: Dict[str, "MemoryManager"] = {}

    def __init__(self):
        # æŒ‰ user_id:agent_id å­˜å‚¨è®°å¿†å®ä¾‹
        self.working_memories: Dict[str, WorkingMemory] = {}
        self.episodic_memories: Dict[str, EpisodicMemory] = {}
        self.semantic_memories: Dict[str, SemanticMemory] = {}
        
        # å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—ä¸çŠ¶æ€è¿½è¸ª
        self.task_queue = asyncio.Queue()
        self.task_states: Dict[str, Dict] = {}
        self._worker_task = None
        
        logger.info("MemoryManager initialized")

    @classmethod
    def get_instance(cls) -> "MemoryManager":
        if "default" not in cls._instances:
            cls._instances["default"] = cls()
        return cls._instances["default"]

    def _ensure_worker_started(self):
        """ç¡®ä¿åå° Worker å·²å¯åŠ¨"""
        if self._worker_task is None or self._worker_task.done():
            try:
                loop = asyncio.get_running_loop()
                self._worker_task = loop.create_task(self._background_worker())
                logger.info("Background Memory Worker started")
            except RuntimeError:
                logger.warning("No running event loop found, worker not started")

    async def _background_worker(self):
        """åå°ä»»åŠ¡å¤„ç†å™¨"""
        logger.info("Memory Worker loop started")
        while True:
            task = await self.task_queue.get()
            task_id = task.get("task_id")
            try:
                task_type = task.get("type")
                user_id = task.get("user_id")
                agent_id = task.get("agent_id")
                
                self.task_states[task_id] = {
                    "status": "processing",
                    "start_time": datetime.now().isoformat()
                }
                
                if task_type == "finalize":
                    await self._process_finalize_task(user_id, agent_id)
                
                self.task_states[task_id]["status"] = "completed"
                self.task_states[task_id]["end_time"] = datetime.now().isoformat()
                logger.info(f"Task {task_id} ({task_type}) completed")
                
            except Exception as e:
                logger.error(f"Error in background task {task_id}: {e}")
                self.task_states[task_id] = {
                    "status": "failed",
                    "error": str(e),
                    "end_time": datetime.now().isoformat()
                }
            finally:
                self.task_queue.task_done()

    async def _process_finalize_task(self, user_id: str, agent_id: str):
        """å®é™…æ‰§è¡Œç»“ç®—é€»è¾‘çš„ç§æœ‰æ–¹æ³•"""
        wm = self._get_working_memory(user_id, agent_id)
        
        # ä»…è·å–å°šæœªç»“ç®—çš„æ–°å¢è¯­æ–™
        new_items = wm.get_unfinalized_details()
        if not new_items:
            logger.info(f"No new items to finalize for {user_id}")
            return

        # ä½¿ç”¨ asyncio.to_thread å°†åŒæ­¥é˜»å¡çš„ LLM/DB æ“ä½œç§»å‡ºä¸»äº‹ä»¶å¾ªç¯ï¼Œé˜²æ­¢é˜»å¡å…¶ä»–ç”¨æˆ·è¯·æ±‚
        # 1. è§¦å‘å‹ç¼©ä¸æå–æµæ°´çº¿ (MTM è½¬åŒ–) - åŸºäºæ–°å¢å†…å®¹
        await asyncio.to_thread(self._handle_compression, user_id, agent_id, new_items)

        # 2. è§¦å‘ç”¨æˆ·ç”»åƒæ›´æ–° (LTM è½¬åŒ–) - åŸºäºæ–°å¢å†…å®¹
        await asyncio.to_thread(self._handle_persona_update, user_id, agent_id, new_items)

        # 3. è§¦å‘é•¿æœŸåŸåˆ™èšç±» (LTM è½¬åŒ–)
        await asyncio.to_thread(self.run_clustering, user_id, agent_id)

        # 4. æ‰§è¡Œåƒåœ¾å›æ”¶ (GC)
        await asyncio.to_thread(self.perform_garbage_collection, user_id, agent_id)

        # 5. æ ‡è®°ä¸ºå·²ç»“ç®—
        wm.mark_finalized()
        
        # 6. æ¸…ç†é™ˆæ—§è®°å¿†ï¼Œä½†ä¿ç•™æœ€è¿‘ 5 è½®ä½œä¸ºçƒ­å¯åŠ¨ä¸Šä¸‹æ–‡ (Cross-session continuity)
        wm.clear(keep_last_n=5)

    def perform_garbage_collection(self, user_id: str, agent_id: str):
        """
        æ‰§è¡Œè®°å¿†æ¸…ç† (GC)
        ç§»é™¤ä½é‡è¦åº¦ä¸”é™ˆæ—§çš„ä¸­æœŸè®°å¿†ï¼Œä¿æŒç³»ç»Ÿè½»é‡
        """
        try:
            em = self._get_episodic_memory(user_id, agent_id)
            count = em.vector_store.count()
            
            # è®¾å®šè½¯ä¸Šé™ï¼Œè¶…è¿‡åˆ™è§¦å‘æ¸…ç†
            SOFT_LIMIT = 1000
            if count > SOFT_LIMIT:
                logger.info(f"Memory GC triggered for {user_id}:{agent_id} (Count: {count})")
                # ç®€å•ç­–ç•¥ï¼šæŒ‰ç´¢å¼•é¡ºåºï¼ˆé€šå¸¸æ˜¯æ—¶é—´é¡ºåºï¼‰åˆ é™¤æœ€æ—§çš„ 10%
                to_delete_count = int(count * 0.1)
                results = em.vector_store.collection.get(
                    limit=to_delete_count,
                    include=["metadatas"]
                )
                if results["ids"]:
                    em.vector_store.delete(ids=results["ids"])
                    logger.info(f"GC deleted {len(results['ids'])} old episodic memories")
        except Exception as e:
            logger.error(f"Memory GC failed: {e}")

    def finalize_session(self, user_id: str, agent_id: str) -> Dict[str, Any]:
        """
        ç»“ç®—å½“å‰ä¼šè¯ (å¼‚æ­¥åŒ–æ”¹ç‰ˆ)ï¼š
        å°†ç»“ç®—ä»»åŠ¡å‹å…¥é˜Ÿåˆ—å¹¶ç«‹å³è¿”å›
        """
        try:
            self._ensure_worker_started()
            task_id = str(uuid.uuid4())
            
            task = {
                "task_id": task_id,
                "type": "finalize",
                "user_id": user_id,
                "agent_id": agent_id,
                "created_at": datetime.now().isoformat()
            }
            
            # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ— (åŒæ­¥æ–¹æ³•ä¸­ä½¿ç”¨ put_nowait)
            self.task_queue.put_nowait(task)
            
            self.task_states[task_id] = {
                "status": "queued",
                "created_at": task["created_at"]
            }
            
            logger.info(f"ğŸ Finalize session queued for user {user_id}. Task ID: {task_id}")
            
            return {
                "status": "accepted",
                "task_id": task_id,
                "message": "Session finalization started in background"
            }
        except Exception as e:
            logger.error(f"Failed to queue finalize session: {e}")
            return {"status": "error", "message": str(e)}

    def add_memory(self, user_id: str, agent_id: str, content: Any, role: str = "user", memory_type: str = "conversation", metadata: Dict = None) -> Dict:
        """æ·»åŠ æ–°è®°å¿†åˆ° Working Memory"""
        wm = self._get_working_memory(user_id, agent_id)
        
        # ç»‘å®šå‹ç¼©å›è°ƒ (å¦‚æœè¿˜æ²¡ç»‘å®š)
        if not wm.compression_callback:
            wm.set_compression_callback(lambda items: self._handle_compression(user_id, agent_id, items))
            
        memory_id = str(uuid.uuid4())
        memory_item = {
            "id": memory_id,
            "content": content,
            "role": role,
            "metadata": metadata or {}
        }
        wm.add(memory_item)
        
        return {
            "status": "success", 
            "memory_id": memory_id,
            "stored_in": ["working_memory"],
            "tokens": wm.total_tokens()
        }

    def get_context(self, user_id: str, agent_id: str, query: str, session_id: str = None) -> Dict:
        """è·å–ä¸‰å±‚è®°å¿†å¤åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å« Token é¢„ç®—æ§åˆ¶"""
        wm = self._get_working_memory(user_id, agent_id)
        em = self._get_episodic_memory(user_id, agent_id)
        sm = self._get_semantic_memory(user_id, agent_id)
        
        budget = settings.TOKEN_BUDGET
        
        # 1. STM - è¿‘æœŸå¯¹è¯ (Working Memory)
        working_items = wm.get_details()
        # WM å·²ç»åœ¨ add æ—¶ä¿è¯äº†æ€»é¢ï¼Œè¿™é‡Œç›´æ¥è·å–
        working_tokens = wm.total_tokens()
        
        # 2. MTM - ç›¸å…³è§è§£ (Episodic Memory)
        episodic_results = em.retrieve(query, top_k=10) # å…ˆå¤šå–ä¸€ç‚¹ç”¨äºé¢„ç®—æ§åˆ¶
        episodic_items = []
        episodic_tokens = 0
        for m in episodic_results:
            item_tokens = tokenizer.count_tokens(m["content"])
            if episodic_tokens + item_tokens > budget.get("episodic_memory", 20000):
                break
            episodic_items.append({
                "content": m["content"], 
                "metadata": m["metadata"], 
                "score": m["score"]
            })
            episodic_tokens += item_tokens
            
        # 3. LTM - ç”»åƒã€åŸåˆ™ä¸ç»éªŒ (Semantic Memory)
        core_principles = sm.get_core_principles()
        principles_tokens = tokenizer.count_tokens(core_principles)
        
        user_persona_data = sm.user_persona
        persona_summary = sm.get_persona_summary()
        persona_tokens = tokenizer.count_tokens(persona_summary)
        
        # è¯­ä¹‰æ£€ç´¢ç›¸å…³ç»éªŒ (Semantic Results)
        semantic_results = sm.retrieve_relevant_experiences(query, top_k=5)
        semantic_items = []
        semantic_tokens = 0
        for m in semantic_results:
            item_tokens = tokenizer.count_tokens(m["content"])
            if semantic_tokens + item_tokens > budget.get("semantic_memory", 500):
                break
            semantic_items.append(f"- {m['content']}")
            semantic_tokens += item_tokens
            
        semantic_context = "\n".join(semantic_items)
        
        # åˆå¹¶ Semantic Memory éƒ¨åˆ†
        full_semantic = f"{persona_summary}\n\nPrinciples:\n{core_principles}\n\nRelevant Experience:\n{semantic_context}"
        total_semantic_tokens = persona_tokens + principles_tokens + semantic_tokens
        
        return {
            "working_memory": working_items,
            "episodic_memory": episodic_items,
            "semantic_memory": full_semantic,
            "user_persona": user_persona_data,
            "core_principles": core_principles,
            "token_usage": {
                "working_memory": working_tokens,
                "core_principles": principles_tokens,
                "episodic_memory": episodic_tokens,
                "semantic_memory": total_semantic_tokens,
                "total": working_tokens + episodic_tokens + total_semantic_tokens
            }
        }

    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–å¼‚æ­¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
        return self.task_states.get(task_id, {"status": "not_found"})

    def get_all_identities(self) -> Dict[str, List[str]]:
        """è·å–ç³»ç»Ÿä¸­å­˜åœ¨çš„æ‰€æœ‰ User å’Œ Agent åˆ—è¡¨ (é€šè¿‡æ‰«ææ•°æ®æ–‡ä»¶)"""
        import os
        from config import settings
        
        users = set()
        agents = set()
        
        # 1. æ‰«æ Working Memory æ–‡ä»¶ (æœ€å‡†ç¡®)
        data_dir = settings.DATA_DIR
        if os.path.exists(data_dir):
            for filename in os.listdir(data_dir):
                if filename.startswith("working_") and filename.endswith(".json"):
                    # working_{user_id}_{agent_id}.json
                    # å»æ‰å‰ç¼€ working_ (8 chars) å’Œåç¼€ .json (5 chars)
                    content_part = filename[8:-5]
                    
                    # å°è¯•åŒ¹é…å·²çŸ¥ Agent
                    known_agents = ["research_agent", "chairman", "market", "macro", "sentiment", "web_search", "receptionist", "researcher"]
                    found_agent = None
                    for ka in known_agents:
                        if content_part.endswith(f"_{ka}"):
                            found_agent = ka
                            break
                    
                    if found_agent:
                        agent_id = found_agent
                        user_id = content_part[: -(len(found_agent) + 1)]
                        if user_id:
                            users.add(user_id)
                            agents.add(agent_id)
                    else:
                        # é™çº§ï¼šå‡è®¾æœ€åä¸€èŠ‚æ˜¯ agent_id
                        parts = content_part.split("_")
                        if len(parts) >= 2:
                            agents.add(parts[-1])
                            users.add("_".join(parts[:-1]))

        # 2. è¡¥å……å†…å­˜ä¸­å½“å‰çš„
        for key in self.working_memories.keys():
            if ":" in key:
                u, a = key.split(":", 1)
                users.add(u)
                agents.add(a)
                
        # 3. ç¡®ä¿é»˜è®¤å€¼å­˜åœ¨
        if not users: users.add("test_user_001")
        if not agents: 
            agents.add("research_agent")
            agents.add("chairman")
                
        return {
            "users": sorted(list(users)),
            "agents": sorted(list(agents))
        }

    def get_stats(self, user_id: str, agent_id: str) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        wm = self._get_working_memory(user_id, agent_id)
        sm = self._get_semantic_memory(user_id, agent_id)

        return {
            "working_memory": {"count": len(wm.items), "tokens": wm.total_tokens()},
            "episodic_memory": {"count": "dynamic"},
            "semantic_memory": {"core_principles": len(sm.core_principles)},
        }

    def _get_working_memory(self, user_id: str, agent_id: str) -> WorkingMemory:
        key = f"{user_id}:{agent_id}"
        if key not in self.working_memories:
            self.working_memories[key] = WorkingMemory(user_id, agent_id)
        return self.working_memories[key]

    def _get_episodic_memory(self, user_id: str, agent_id: str) -> EpisodicMemory:
        key = f"{user_id}:{agent_id}"
        if key not in self.episodic_memories:
            self.episodic_memories[key] = EpisodicMemory(user_id, agent_id)
        return self.episodic_memories[key]

    def _get_semantic_memory(self, user_id: str, agent_id: str) -> SemanticMemory:
        key = f"{user_id}:{agent_id}"
        if key not in self.semantic_memories:
            self.semantic_memories[key] = SemanticMemory(user_id, agent_id)
        return self.semantic_memories[key]

    def _handle_compression(self, user_id: str, agent_id: str, items: List[Dict]):
        """å¤„ç†è®°å¿†å‹ç¼©ä¸ä¸­æœŸè®°å¿†æå–"""
        em = self._get_episodic_memory(user_id, agent_id)
        
        # æå–æŠ•èµ„è§è§£
        full_text = "\n".join([f"{item['role']}: {item['content']}" for item in items])
        insight = extractor.extract_investment_insight(full_text)
        
        if insight and insight.get("symbol"):
            # å°†å…³é”®ç»´åº¦å­˜å…¥ metadata ä»¥ä¾¿åç»­åˆ†æ
            metadata = {
                "symbol": insight["symbol"],
                "viewpoint": insight.get("viewpoint"),
                "confidence": insight.get("confidence", 0.5)
            }
            em.add_event(
                event_type="InvestmentInsight",
                content=insight,
                entities=[insight["symbol"]],
                importance=insight.get("confidence", 0.5),
                metadata_extra=metadata # å‡è®¾ add_event æ”¯æŒè¿™ä¸ª
            )
            logger.info(f"Extracted investment insight for {insight['symbol']} ({insight.get('viewpoint')})")

        # æå–é€šç”¨äº‹ä»¶
        event = extractor.extract(full_text)
        if event:
            em.add_event(
                event_type=event.get("event_type", "General"),
                content=event,
                entities=event.get("entities", []),
                importance=0.4
            )

    def _handle_persona_update(self, user_id: str, agent_id: str, items: List[Dict]):
        """ä»å¯¹è¯ä¸­æå–å¹¶æ›´æ–°ç”¨æˆ·ç”»åƒ"""
        sm = self._get_semantic_memory(user_id, agent_id)
        conversation_text = "\n".join([f"{item['role']}: {item['content']}" for item in items])
        
        new_traits = extractor.extract_user_persona(conversation_text)
        if new_traits:
            sm.update_persona(new_traits)

    def run_clustering(self, user_id: str, agent_id: str):
        """è¿è¡Œèšç±»ç®—æ³•æå–é•¿æœŸåŸåˆ™"""
        clusterer = ConceptCluster(user_id, agent_id)
        principles = clusterer.cluster_and_abstract()
        
        if principles:
            sm = self._get_semantic_memory(user_id, agent_id)
            for p in principles:
                sm.add_core_principle(p)
