# Memory System - Detailed Design Document

## ğŸ“‹ ç›®å½•

- [è®¾è®¡ç›®æ ‡](#è®¾è®¡ç›®æ ‡)
- [ä¸‰å±‚è®°å¿†è¯¦è§£](#ä¸‰å±‚è®°å¿†è¯¦è§£)
- [è®°å¿†æµè½¬æœºåˆ¶](#è®°å¿†æµè½¬æœºåˆ¶)
- [æ£€ç´¢ç­–ç•¥](#æ£€ç´¢ç­–ç•¥)
- [å‹ç¼©ç®—æ³•](#å‹ç¼©ç®—æ³•)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å®‰å…¨ä¸éš”ç¦»](#å®‰å…¨ä¸éš”ç¦»)

---

## ğŸ¯ è®¾è®¡ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡

1. **ç‹¬ç«‹æ€§**ï¼šå®Œå…¨è§£è€¦ï¼Œé€šè¿‡APIé€šä¿¡
2. **é«˜æ€§èƒ½**ï¼šæ£€ç´¢å»¶è¿Ÿ <100ms
3. **å¯æ‰©å±•**ï¼šæ”¯æŒ100+ Agentå¹¶å‘
4. **æ™ºèƒ½åŒ–**ï¼šè‡ªåŠ¨å‹ç¼©å’Œä¼˜åŒ–
5. **å¯è¿½æº¯**ï¼šå®Œæ•´çš„è®°å¿†æº¯æºé“¾

### éåŠŸèƒ½æ€§éœ€æ±‚

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | æµ‹é‡æ–¹æ³• |
|------|--------|---------|
| APIå“åº”æ—¶é—´ | <100ms (P95) | Prometheusç›‘æ§ |
| æ£€ç´¢å‡†ç¡®ç‡ | >85% | äººå·¥æ ‡æ³¨è¯„ä¼° |
| å¹¶å‘æ”¯æŒ | 100 QPS | å‹åŠ›æµ‹è¯• |
| æ•°æ®æŒä¹…åŒ– | 99.9% | å®šæœŸå¤‡ä»½éªŒè¯ |
| å†…å­˜å ç”¨ | <2GB | ç³»ç»Ÿç›‘æ§ |

---

## ğŸ§  ä¸‰å±‚è®°å¿†è¯¦è§£

### 1. è¿‘æœŸè®°å¿†ï¼ˆWorking Memoryï¼‰

#### è®¾è®¡åŸç†

åŸºäº**æ“ä½œç³»ç»Ÿè™šæ‹Ÿå†…å­˜**çš„æ€æƒ³ï¼Œè¿‘æœŸè®°å¿†ç›¸å½“äº"ä¸»å†…å­˜"ï¼Œå­˜å‚¨æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ã€‚

#### æ•°æ®ç»“æ„

```python
from collections import deque
from dataclasses import dataclass
from datetime import datetime

@dataclass
class WorkingMemoryItem:
    """è¿‘æœŸè®°å¿†å•å…ƒ"""
    id: str
    agent_id: str
    session_id: str
    timestamp: datetime
    role: str  # "user" | "agent" | "system"
    content: str
    tokens: int
    importance: float  # 0.0-1.0
    protected: bool = False  # æ˜¯å¦å—ä¿æŠ¤ä¸è¢«æ·˜æ±°

class WorkingMemory:
    """è¿‘æœŸè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self, max_items: int = 50, max_tokens: int = 8000):
        self.items: deque[WorkingMemoryItem] = deque(maxlen=max_items)
        self.max_tokens = max_tokens
        self.agent_sessions: dict[str, deque] = {}  # agent_id -> session memories
    
    def add(self, item: WorkingMemoryItem) -> None:
        """æ·»åŠ è®°å¿†ï¼Œè‡ªåŠ¨å¤„ç†æº¢å‡º"""
        # æ£€æŸ¥Tokené™åˆ¶
        while self._total_tokens() + item.tokens > self.max_tokens:
            self._compress_oldest()
        
        self.items.append(item)
    
    def _compress_oldest(self) -> None:
        """å‹ç¼©æœ€æ—§çš„Næ¡è®°å¿†"""
        # æ‰¾åˆ°æœ€æ—§çš„5æ¡éä¿æŠ¤è®°å¿†
        to_compress = []
        for item in list(self.items):
            if not item.protected and len(to_compress) < 5:
                to_compress.append(item)
                self.items.remove(item)
        
        if to_compress:
            # ç”Ÿæˆæ‘˜è¦
            summary = self._summarize(to_compress)
            # è½¬ç§»åˆ°ä¸­æœŸè®°å¿†
            self._move_to_episodic(to_compress)
            # æ’å…¥æ‘˜è¦åˆ°é˜Ÿé¦–
            self.items.appendleft(summary)
    
    def to_context(self) -> str:
        """è½¬æ¢ä¸ºLLMä¸Šä¸‹æ–‡"""
        return "\n".join([
            f"{item.role}: {item.content}"
            for item in self.items
        ])
```

#### Tokenç®¡ç†ç­–ç•¥

```python
class TokenBudgetManager:
    """Tokené¢„ç®—ç®¡ç†å™¨"""
    
    BUDGET = {
        "system_base": 800,
        "core_principles": 500,
        "working_memory": 8000,
        "episodic_memory": 2000,
        "semantic_memory": 500,
        "tools": 2000,
        "response": 4000
    }
    
    def allocate(self, memory_type: str) -> int:
        """åˆ†é…Tokené¢„ç®—"""
        return self.BUDGET.get(memory_type, 0)
    
    def check_overflow(self, current_tokens: int, budget: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…å‡ºé¢„ç®—"""
        return current_tokens > budget
```

---

### 2. ä¸­æœŸè®°å¿†ï¼ˆEpisodic Memoryï¼‰

#### è®¾è®¡åŸç†

åŸºäº**äº‹ä»¶æº¯æºï¼ˆEvent Sourcingï¼‰**æ¨¡å¼ï¼Œå­˜å‚¨ç»“æ„åŒ–çš„äº‹ä»¶è€ŒéåŸå§‹å¯¹è¯ã€‚

#### äº‹ä»¶æŠ½å–

```python
from typing import List, Dict, Tuple

class EventExtractor:
    """äº‹ä»¶æŠ½å–å™¨"""
    
    def extract(self, conversation: str) -> List[EpisodicEvent]:
        """ä»å¯¹è¯ä¸­æå–ç»“æ„åŒ–äº‹ä»¶"""
        # ä½¿ç”¨LLMæå–äº‹ä»¶
        prompt = f"""
ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–å…³é”®äº‹ä»¶ï¼Œæ ¼å¼åŒ–ä¸ºJSONï¼š

å¯¹è¯ï¼š
{conversation}

æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. event_type: äº‹ä»¶ç±»å‹ï¼ˆstock_analysis/news_search/user_preferenceç­‰ï¼‰
2. entities: æ¶‰åŠçš„å®ä½“åˆ—è¡¨
3. relations: ä¸‰å…ƒç»„å…³ç³»åˆ—è¡¨ (subject, predicate, object)
4. key_findings: å…³é”®å‘ç°ï¼ˆdictæ ¼å¼ï¼‰
5. importance: é‡è¦æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
"""
        
        # è°ƒç”¨LLM
        response = self.llm.generate(prompt)
        events = self._parse_events(response)
        
        return events

@dataclass
class EpisodicEvent:
    """ä¸­æœŸè®°å¿†äº‹ä»¶"""
    id: str
    agent_id: str
    event_type: str
    entities: List[str]
    relations: List[Tuple[str, str, str]]  # (subject, predicate, object)
    key_findings: Dict
    timestamp: datetime
    importance: float
    embedding: List[float]
    access_count: int = 0
    last_accessed: datetime = None
```

#### çŸ¥è¯†å›¾è°±æ„å»º

```python
import networkx as nx

class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±ç®¡ç†å™¨"""
    
    def __init__(self):
        self.graph = nx.MultiDiGraph()
    
    def add_event(self, event: EpisodicEvent) -> None:
        """å°†äº‹ä»¶æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±"""
        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        for entity in event.entities:
            if not self.graph.has_node(entity):
                self.graph.add_node(entity, type="entity")
        
        # æ·»åŠ å…³ç³»è¾¹
        for subject, predicate, obj in event.relations:
            self.graph.add_edge(
                subject, obj,
                relation=predicate,
                event_id=event.id,
                timestamp=event.timestamp,
                weight=event.importance
            )
    
    def find_path(self, start: str, end: str, max_depth: int = 3) -> List:
        """æŸ¥æ‰¾å®ä½“é—´çš„è·¯å¾„"""
        try:
            paths = nx.all_simple_paths(
                self.graph, start, end, cutoff=max_depth
            )
            return list(paths)
        except nx.NetworkXNoPath:
            return []
    
    def expand_neighbors(self, entities: List[str], max_depth: int = 2) -> List[str]:
        """æ‰©å±•ç›¸å…³å®ä½“"""
        expanded = set(entities)
        for entity in entities:
            if self.graph.has_node(entity):
                neighbors = nx.single_source_shortest_path_length(
                    self.graph, entity, cutoff=max_depth
                )
                expanded.update(neighbors.keys())
        return list(expanded)
```

#### æ—¶é—´è¡°å‡æœºåˆ¶

```python
import math
from datetime import datetime, timedelta

class TimeDecayCalculator:
    """æ—¶é—´è¡°å‡è®¡ç®—å™¨"""
    
    def __init__(self, decay_rate: float = 0.1):
        """
        decay_rate: è¡°å‡ç‡Î»ï¼Œè¶Šå¤§è¡°å‡è¶Šå¿«
        - 0.05: ç¼“æ…¢è¡°å‡ï¼ˆ30å¤©è¡°å‡åˆ°22%ï¼‰
        - 0.1: ä¸­ç­‰è¡°å‡ï¼ˆ30å¤©è¡°å‡åˆ°5%ï¼‰
        - 0.2: å¿«é€Ÿè¡°å‡ï¼ˆ30å¤©è¡°å‡åˆ°0.25%ï¼‰
        """
        self.decay_rate = decay_rate
    
    def calculate(self, base_score: float, created_at: datetime) -> float:
        """
        è®¡ç®—è¡°å‡åçš„åˆ†æ•°
        
        å…¬å¼: score = base_score * e^(-Î»t)
        """
        days_passed = (datetime.now() - created_at).days
        return base_score * math.exp(-self.decay_rate * days_passed)
    
    def boost_by_access(self, score: float, access_count: int) -> float:
        """æ ¹æ®è®¿é—®æ¬¡æ•°æå‡åˆ†æ•°"""
        # è®¿é—®é¢‘ç‡è¶Šé«˜ï¼Œåˆ†æ•°æå‡è¶Šå¤šï¼ˆä½†æœ‰ä¸Šé™ï¼‰
        boost = min(0.3, 0.1 * math.log(access_count + 1))
        return score * (1 + boost)
```

---

### 3. é•¿æœŸè®°å¿†ï¼ˆSemantic Memoryï¼‰

#### è®¾è®¡åŸç†

åŸºäº**çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰**ï¼Œå°†å¤§é‡ä¸­æœŸè®°å¿†å‹ç¼©ä¸ºé«˜è´¨é‡çš„æŠ½è±¡çŸ¥è¯†ã€‚

#### åˆ†ç±»å­˜å‚¨

```python
from enum import Enum

class SemanticCategory(Enum):
    """é•¿æœŸè®°å¿†åˆ†ç±»"""
    CORE_PRINCIPLE = "core_principle"      # æ ¸å¿ƒåŸåˆ™ï¼ˆå›ºå®šåŠ è½½ï¼‰
    EXPERIENCE_RULE = "experience_rule"    # ç»éªŒæ³•åˆ™ï¼ˆåŠ¨æ€æ£€ç´¢ï¼‰
    USER_PREFERENCE = "user_preference"    # ç”¨æˆ·åå¥½
    DOMAIN_KNOWLEDGE = "domain_knowledge"  # é¢†åŸŸçŸ¥è¯†

@dataclass
class SemanticKnowledge:
    """é•¿æœŸè®°å¿†çŸ¥è¯†å•å…ƒ"""
    id: str
    agent_id: str
    category: SemanticCategory
    title: str
    content: str
    applicable_scenarios: List[str]
    confidence: float
    source_events: List[str]  # æº¯æºåˆ°ä¸­æœŸè®°å¿†
    created_at: datetime
    embedding: List[float]
    importance: float = 0.8

class SemanticMemory:
    """é•¿æœŸè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self):
        self.core_db: Dict[str, List[SemanticKnowledge]] = {}  # agent_id -> cores
        self.experience_db = ChromaDB(collection="experiences")
    
    def get_core_principles(self, agent_id: str, budget: int = 500) -> str:
        """è·å–æ ¸å¿ƒåŸåˆ™ï¼ˆå›ºå®šåŠ è½½ï¼‰"""
        cores = self.core_db.get(agent_id, [])
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_cores = sorted(cores, key=lambda x: x.importance, reverse=True)
        
        # æ§åˆ¶Tokené¢„ç®—
        result = []
        tokens = 0
        for core in sorted_cores:
            core_tokens = count_tokens(core.content)
            if tokens + core_tokens > budget:
                break
            result.append(f"- {core.content}")
            tokens += core_tokens
        
        return "\n".join(result)
    
    def retrieve_relevant(self, query: str, agent_id: str, budget: int = 500) -> str:
        """æ£€ç´¢ç›¸å…³ç»éªŒï¼ˆåŠ¨æ€åŠ è½½ï¼‰"""
        # å‘é‡æ£€ç´¢
        results = self.experience_db.query(
            query_embeddings=[embed(query)],
            where={"agent_id": agent_id},
            n_results=10
        )
        
        # æ§åˆ¶Tokené¢„ç®—
        final = []
        tokens = 0
        for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
            doc_tokens = count_tokens(doc)
            if tokens + doc_tokens > budget:
                break
            final.append(f"- {doc} (ç½®ä¿¡åº¦: {metadata['confidence']})")
            tokens += doc_tokens
        
        return "\n".join(final)
```

---

## ğŸ”„ è®°å¿†æµè½¬æœºåˆ¶

### æµè½¬è§¦å‘æ¡ä»¶

```python
class MemoryFlowController:
    """è®°å¿†æµè½¬æ§åˆ¶å™¨"""
    
    def should_compress_to_episodic(self, working_memory: WorkingMemory) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©åˆ°ä¸­æœŸè®°å¿†"""
        # æ¡ä»¶1: Tokenè¶…é™
        if working_memory.total_tokens() > working_memory.max_tokens:
            return True
        
        # æ¡ä»¶2: æ¡ç›®æ•°è¶…é™
        if len(working_memory.items) >= working_memory.max_items:
            return True
        
        return False
    
    def should_compress_to_semantic(self, episodic_memory) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å‹ç¼©åˆ°é•¿æœŸè®°å¿†"""
        # æ¡ä»¶1: æ—¶é—´è§¦å‘ï¼ˆæ¯7å¤©ï¼‰
        if self.days_since_last_compression() >= 7:
            return True
        
        # æ¡ä»¶2: å®¹é‡è§¦å‘ï¼ˆ>5000æ¡ï¼‰
        if episodic_memory.count() > 5000:
            return True
        
        # æ¡ä»¶3: å†—ä½™åº¦è§¦å‘ï¼ˆ>30%é‡å¤ï¼‰
        if self.calculate_redundancy(episodic_memory) > 0.3:
            return True
        
        return False
```

### å‹ç¼©æµç¨‹

```mermaid
graph TB
    A[è§¦å‘å‹ç¼©] --> B{å‹ç¼©ç±»å‹}
    B -->|è¿‘æœŸâ†’ä¸­æœŸ| C[æå–æœ€æ—§5æ¡]
    B -->|ä¸­æœŸâ†’é•¿æœŸ| D[æå–30å¤©å†…è®°å¿†]
    
    C --> E[äº‹ä»¶æŠ½å–]
    E --> F[æ„å»ºçŸ¥è¯†å›¾è°±]
    F --> G[å­˜å…¥Chroma]
    
    D --> H[k-Meansèšç±»]
    H --> I[LLMè‡ªçœæ€»ç»“]
    I --> J[å»é‡éªŒè¯]
    J --> K[å­˜å…¥é•¿æœŸè®°å¿†]
    K --> L[åˆ é™¤åŸå§‹ä¸­æœŸè®°å¿†]
```

---

## ğŸ” æ£€ç´¢ç­–ç•¥

### æ··åˆæ£€ç´¢ç®—æ³•

```python
class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""
    
    def retrieve(
        self,
        query: str,
        agent_id: str,
        top_k: int = 5,
        budget: int = 2000
    ) -> List[EpisodicEvent]:
        """
        æ··åˆæ£€ç´¢æµç¨‹ï¼š
        1. å‘é‡ç²—æ’ï¼ˆTop 20ï¼‰
        2. æ—¶é—´è¡°å‡ + é‡è¦æ€§åŠ æƒ
        3. å›¾æ‰©å±•ç›¸å…³å®ä½“
        4. ç²¾æ’ + Tokené¢„ç®—æ§åˆ¶
        """
        
        # 1. å‘é‡æ£€ç´¢ï¼ˆç²—æ’ï¼‰
        vector_results = self.vector_db.query(
            query_embeddings=[embed(query)],
            where={"agent_id": agent_id},
            n_results=20
        )
        
        # 2. åŠ æƒè¯„åˆ†
        scored_results = []
        for doc, metadata, distance in zip(
            vector_results['documents'][0],
            vector_results['metadatas'][0],
            vector_results['distances'][0]
        ):
            # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦è·ç¦»è½¬ç›¸ä¼¼åº¦ï¼‰
            similarity = 1 - distance
            
            # æ—¶é—´è¡°å‡
            recency = self.decay_calculator.calculate(
                1.0, metadata['timestamp']
            )
            
            # è®¿é—®é¢‘ç‡åŠ æƒ
            access_boost = self.decay_calculator.boost_by_access(
                1.0, metadata['access_count']
            )
            
            # ç»¼åˆè¯„åˆ†
            score = (
                similarity * 0.6 +
                metadata['importance'] * 0.3 +
                recency * 0.1
            ) * access_boost
            
            scored_results.append({
                'doc': doc,
                'metadata': metadata,
                'score': score
            })
        
        # 3. å›¾æ‰©å±•
        top_entities = self._extract_entities(scored_results[:5])
        expanded_entities = self.knowledge_graph.expand_neighbors(
            top_entities, max_depth=2
        )
        
        # æŸ¥æ‰¾åŒ…å«æ‰©å±•å®ä½“çš„è®°å¿†
        expanded_results = self._find_by_entities(expanded_entities)
        
        # 4. åˆå¹¶å»é‡
        all_results = self._merge_and_deduplicate(
            scored_results, expanded_results
        )
        
        # 5. ç²¾æ’ + Tokené¢„ç®—æ§åˆ¶
        final_results = []
        tokens = 0
        for result in sorted(all_results, key=lambda x: x['score'], reverse=True):
            result_tokens = count_tokens(result['doc'])
            if tokens + result_tokens > budget:
                break
            final_results.append(result)
            tokens += result_tokens
        
        return final_results[:top_k]
```

### ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢

```python
class ContextAwareRetriever:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢å™¨"""
    
    def retrieve(
        self,
        query: str,
        agent_id: str,
        task_type: str,  # "stock_analysis" / "news_search" ç­‰
        top_k: int = 5
    ) -> List:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æ£€ç´¢ç­–ç•¥"""
        
        # åŸºç¡€æ£€ç´¢
        base_results = self.hybrid_retriever.retrieve(query, agent_id, top_k=10)
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´æƒé‡
        for result in base_results:
            # å¦‚æœè®°å¿†çš„ç±»å‹ä¸å½“å‰ä»»åŠ¡åŒ¹é…ï¼Œæå‡æƒé‡
            if result['metadata'].get('event_type') == task_type:
                result['score'] *= 1.5
            
            # å¦‚æœæ˜¯åŒä¸€Agentçš„è®°å¿†ï¼Œæå‡æƒé‡
            if result['metadata'].get('agent_id') == agent_id:
                result['score'] *= 1.3
        
        # é‡æ–°æ’åº
        sorted_results = sorted(base_results, key=lambda x: x['score'], reverse=True)
        
        return sorted_results[:top_k]
```

---

## ğŸ—œï¸ å‹ç¼©ç®—æ³•

### k-Meansèšç±»å‹ç¼©

```python
from sklearn.cluster import KMeans
import numpy as np

class MemoryCompressor:
    """è®°å¿†å‹ç¼©å™¨"""
    
    def compress(
        self,
        memories: List[EpisodicEvent],
        n_clusters: int = 10
    ) -> List[SemanticKnowledge]:
        """
        ä½¿ç”¨k-Meansèšç±»å‹ç¼©è®°å¿†
        
        æµç¨‹ï¼š
        1. æå–å‘é‡åµŒå…¥
        2. k-Meansèšç±»
        3. LLMæ€»ç»“æ¯ä¸ªç°‡
        4. ç”Ÿæˆé•¿æœŸè®°å¿†
        """
        
        # 1. æå–åµŒå…¥
        embeddings = np.array([m.embedding for m in memories])
        
        # 2. k-Meansèšç±»
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(embeddings)
        
        # 3. æŒ‰ç°‡åˆ†ç»„
        clusters = {}
        for i, label in enumerate(labels):
            if label not in clusters:
                clusters[label] = []
            clusters[label].append(memories[i])
        
        # 4. LLMæ€»ç»“æ¯ä¸ªç°‡
        semantic_memories = []
        for cluster_id, cluster_memories in clusters.items():
            summary = self._summarize_cluster(cluster_memories)
            semantic_memories.append(summary)
        
        return semantic_memories
    
    def _summarize_cluster(self, memories: List[EpisodicEvent]) -> SemanticKnowledge:
        """ä½¿ç”¨LLMæ€»ç»“ä¸€ä¸ªç°‡çš„è®°å¿†"""
        
        # æ„å»ºæç¤º
        memory_texts = [
            f"- {m.event_type}: {m.key_findings} (é‡è¦æ€§: {m.importance})"
            for m in memories
        ]
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªè®°å¿†å‹ç¼©ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯ {len(memories)} æ¡ç›¸å…³çš„è®°å¿†äº‹ä»¶ï¼š

{chr(10).join(memory_texts)}

è¯·æ€»ç»“å‡º3-5æ¡å¯å¤ç”¨çš„ç»éªŒæ³•åˆ™ï¼Œæ ¼å¼ï¼š
1. **ç»éªŒæ³•åˆ™**: [å…·ä½“å†…å®¹]
   - é€‚ç”¨åœºæ™¯: [åœºæ™¯åˆ—è¡¨]
   - ç½®ä¿¡åº¦: [0-1çš„æ•°å€¼]
   - ä¾æ®: [åŸºäºå¤šå°‘æ¡è®°å¿†]
"""
        
        # è°ƒç”¨LLM
        response = self.llm.generate(prompt)
        
        # è§£æå“åº”
        knowledge = SemanticKnowledge(
            id=generate_id(),
            agent_id=memories[0].agent_id,
            category=SemanticCategory.EXPERIENCE_RULE,
            title=f"ç»éªŒæ€»ç»“ - ç°‡{cluster_id}",
            content=response,
            applicable_scenarios=self._extract_scenarios(response),
            confidence=self._calculate_confidence(memories),
            source_events=[m.id for m in memories],
            created_at=datetime.now(),
            embedding=self._generate_embedding(response)
        )
        
        return knowledge
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥

```python
from functools import lru_cache
import redis

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379)
        self.ttl = 3600  # 1å°æ—¶
    
    @lru_cache(maxsize=1000)
    def get_embedding(self, text: str) -> List[float]:
        """ç¼“å­˜åµŒå…¥å‘é‡"""
        # å…ˆæŸ¥Redis
        cached = self.redis_client.get(f"emb:{hash(text)}")
        if cached:
            return json.loads(cached)
        
        # ç”Ÿæˆæ–°åµŒå…¥
        embedding = self._generate_embedding(text)
        
        # å­˜å…¥Redis
        self.redis_client.setex(
            f"emb:{hash(text)}",
            self.ttl,
            json.dumps(embedding)
        )
        
        return embedding
```

### æ‰¹é‡å¤„ç†

```python
class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.pending_writes = []
    
    def add_memory(self, memory: EpisodicEvent) -> None:
        """æ·»åŠ è®°å¿†åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—"""
        self.pending_writes.append(memory)
        
        if len(self.pending_writes) >= self.batch_size:
            self.flush()
    
    def flush(self) -> None:
        """æ‰¹é‡å†™å…¥æ•°æ®åº“"""
        if not self.pending_writes:
            return
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥
        texts = [m.key_findings for m in self.pending_writes]
        embeddings = self._batch_embed(texts)
        
        # æ‰¹é‡å†™å…¥Chroma
        self.vector_db.add(
            documents=texts,
            embeddings=embeddings,
            metadatas=[m.to_dict() for m in self.pending_writes]
        )
        
        self.pending_writes.clear()
```

---

## ğŸ”’ å®‰å…¨ä¸éš”ç¦»

### Agentéš”ç¦»

```python
class AgentIsolation:
    """Agentéš”ç¦»ç®¡ç†å™¨"""
    
    def __init__(self):
        self.permissions = {}  # agent_id -> permissions
    
    def check_access(self, agent_id: str, memory_id: str) -> bool:
        """æ£€æŸ¥Agentæ˜¯å¦æœ‰æƒè®¿é—®æŸæ¡è®°å¿†"""
        memory = self.get_memory(memory_id)
        
        # åªèƒ½è®¿é—®è‡ªå·±çš„è®°å¿†
        if memory.agent_id != agent_id:
            # é™¤éæœ‰è·¨Agentå…±äº«æƒé™
            if not self.has_cross_agent_permission(agent_id, memory.agent_id):
                return False
        
        return True
    
    def has_cross_agent_permission(self, requester: str, owner: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è·¨Agentè®¿é—®æƒé™"""
        # Chairmanå¯ä»¥è®¿é—®æ‰€æœ‰Agentçš„è®°å¿†
        if requester == "chairman_agent":
            return True
        
        # æ£€æŸ¥æ˜¾å¼æˆæƒ
        perms = self.permissions.get(requester, {})
        return owner in perms.get('can_access', [])
```

### æ•°æ®åŠ å¯†

```python
from cryptography.fernet import Fernet

class MemoryEncryption:
    """è®°å¿†åŠ å¯†å™¨"""
    
    def __init__(self, key: bytes):
        self.cipher = Fernet(key)
    
    def encrypt_memory(self, memory: dict) -> dict:
        """åŠ å¯†æ•æ„Ÿå­—æ®µ"""
        sensitive_fields = ['content', 'key_findings']
        
        for field in sensitive_fields:
            if field in memory:
                encrypted = self.cipher.encrypt(
                    json.dumps(memory[field]).encode()
                )
                memory[field] = encrypted.decode()
        
        return memory
    
    def decrypt_memory(self, memory: dict) -> dict:
        """è§£å¯†è®°å¿†"""
        sensitive_fields = ['content', 'key_findings']
        
        for field in sensitive_fields:
            if field in memory:
                decrypted = self.cipher.decrypt(
                    memory[field].encode()
                )
                memory[field] = json.loads(decrypted.decode())
        
        return memory
```

---

## ğŸ“Š ç›‘æ§ä¸æ—¥å¿—

### æ€§èƒ½ç›‘æ§

```python
from prometheus_client import Counter, Histogram, Gauge

class MemoryMetrics:
    """è®°å¿†ç³»ç»ŸæŒ‡æ ‡"""
    
    # è®¡æ•°å™¨
    memory_add_total = Counter('memory_add_total', 'Total memory additions')
    memory_retrieve_total = Counter('memory_retrieve_total', 'Total memory retrievals')
    
    # ç›´æ–¹å›¾
    retrieve_latency = Histogram('retrieve_latency_seconds', 'Retrieval latency')
    compression_latency = Histogram('compression_latency_seconds', 'Compression latency')
    
    # ä»ªè¡¨ç›˜
    working_memory_size = Gauge('working_memory_size', 'Working memory size')
    episodic_memory_size = Gauge('episodic_memory_size', 'Episodic memory size')
    semantic_memory_size = Gauge('semantic_memory_size', 'Semantic memory size')
```

---

## ğŸ¯ æ€»ç»“

æœ¬è®¾è®¡æ–‡æ¡£è¯¦ç»†æè¿°äº†ä¸‰å±‚è®°å¿†ç³»ç»Ÿçš„æ ¸å¿ƒæœºåˆ¶ï¼š

1. **è¿‘æœŸè®°å¿†**ï¼šåŸºäºè™šæ‹Ÿå†…å­˜æ€æƒ³ï¼ŒåŒé‡é™åˆ¶ï¼ˆæ¡ç›®+Tokenï¼‰
2. **ä¸­æœŸè®°å¿†**ï¼šåŸºäºäº‹ä»¶æº¯æºï¼Œå‘é‡+å›¾æ··åˆå­˜å‚¨
3. **é•¿æœŸè®°å¿†**ï¼šåŸºäºçŸ¥è¯†è’¸é¦ï¼Œæ ¸å¿ƒå›ºå®š+ä¸“ä¸šåŠ¨æ€

å…³é”®åˆ›æ–°ç‚¹ï¼š
- âœ… äº‹ä»¶åŒ–å­˜å‚¨ï¼ˆè€Œéå…¨é‡å¯¹è¯ï¼‰
- âœ… æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å›¾+æ—¶é—´è¡°å‡ï¼‰
- âœ… æ™ºèƒ½å‹ç¼©ï¼ˆk-Means+LLMè‡ªçœï¼‰
- âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼ˆä»»åŠ¡ç±»å‹åŠ æƒï¼‰
- âœ… å®Œå…¨è§£è€¦ï¼ˆç‹¬ç«‹APIæœåŠ¡ï¼‰

ä¸‹ä¸€æ­¥ï¼šå‚è€ƒ [APIæ–‡æ¡£](docs/API.md) äº†è§£æ¥å£ç»†èŠ‚ï¼Œæˆ–æŸ¥çœ‹ [é›†æˆæŒ‡å—](docs/INTEGRATION.md) å¼€å§‹é›†æˆã€‚
